{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "independent-exclusive",
   "metadata": {},
   "source": [
    "이번 실습에서는 NLTK의 불용어(stopwords)를 사용할 것이다. \n",
    "`NTLK`와 `NLTK` 데이터셋이 설치되어 있지 않은 환경이라면, 우선 `NLTK`를 설치하고 `NTLK`의 데이터셋을 다운로드하자.\n",
    "\n",
    "`NLTK`는 Natural Language Toolkit의 축약어로 영어 기호, 통계, 자연어 처리를 위한 라이브러리이다. \n",
    "이 `NLTK`에는 I, my, me, over, 조사, 접미사와 같이 문장에는 자주 등장하지만, 의미를 분석하고 요약하는 데는 거의 의미가 없는 100여개의 불용어가 미리 정리되어 있다. \\\n",
    "이를 이용해 다운로드한 리뷰 파일에서 불용어를 제거하는 작업을 진행할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conscious-remains",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "previous-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-sword",
   "metadata": {},
   "source": [
    "전체 데이터 중 Summary 열과 Text 열만 훈련에 사용할 거라, 이 두 개의 열만 별도로 저장하고, 다시 data를 정의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spanish-greeting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30937</th>\n",
       "      <td>These chips are the best. Better than regular ...</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77683</th>\n",
       "      <td>I really enjoyed the taste of this carbonated ...</td>\n",
       "      <td>Good taste!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20702</th>\n",
       "      <td>Although the description said 3 large packets,...</td>\n",
       "      <td>Only received one worn packet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68522</th>\n",
       "      <td>My favorite tea ever! Its great cold, hot, wit...</td>\n",
       "      <td>My favorite tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27143</th>\n",
       "      <td>I love this gum and its the best deal out ther...</td>\n",
       "      <td>Free shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50295</th>\n",
       "      <td>Buy more than one box - these disappear fast! ...</td>\n",
       "      <td>Rich, but delicate,  hazelnut (filbert) cookie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58890</th>\n",
       "      <td>These are great crackers.  My little guy likes...</td>\n",
       "      <td>Little ones like them...and NO SOY!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45492</th>\n",
       "      <td>These are great little crackers to go with sou...</td>\n",
       "      <td>Great little crackers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73074</th>\n",
       "      <td>Out of all the commercial grade dog foods, thi...</td>\n",
       "      <td>fine for the first week until you find the bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33313</th>\n",
       "      <td>I have decided to write this review after read...</td>\n",
       "      <td>Energy and antioxidant booster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85105</th>\n",
       "      <td>What arrived is not a true cultures for health...</td>\n",
       "      <td>I question sanitation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70304</th>\n",
       "      <td>I have been a big fan and consumer of the Lipt...</td>\n",
       "      <td>HORRIBLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81660</th>\n",
       "      <td>I don't just limit this sauce to dipping in gy...</td>\n",
       "      <td>This stuff is amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49340</th>\n",
       "      <td>There are few foods I don't like, but I couldn...</td>\n",
       "      <td>Terrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71980</th>\n",
       "      <td>These have better ingredients than regular Mil...</td>\n",
       "      <td>Good biscuits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "30937  These chips are the best. Better than regular ...   \n",
       "77683  I really enjoyed the taste of this carbonated ...   \n",
       "20702  Although the description said 3 large packets,...   \n",
       "68522  My favorite tea ever! Its great cold, hot, wit...   \n",
       "27143  I love this gum and its the best deal out ther...   \n",
       "50295  Buy more than one box - these disappear fast! ...   \n",
       "58890  These are great crackers.  My little guy likes...   \n",
       "45492  These are great little crackers to go with sou...   \n",
       "73074  Out of all the commercial grade dog foods, thi...   \n",
       "33313  I have decided to write this review after read...   \n",
       "85105  What arrived is not a true cultures for health...   \n",
       "70304  I have been a big fan and consumer of the Lipt...   \n",
       "81660  I don't just limit this sauce to dipping in gy...   \n",
       "49340  There are few foods I don't like, but I couldn...   \n",
       "71980  These have better ingredients than regular Mil...   \n",
       "\n",
       "                                                 Summary  \n",
       "30937                                          excellent  \n",
       "77683                                        Good taste!  \n",
       "20702                      Only received one worn packet  \n",
       "68522                                    My favorite tea  \n",
       "27143                                      Free shipping  \n",
       "50295  Rich, but delicate,  hazelnut (filbert) cookie...  \n",
       "58890                Little ones like them...and NO SOY!  \n",
       "45492                              Great little crackers  \n",
       "73074  fine for the first week until you find the bes...  \n",
       "33313                     Energy and antioxidant booster  \n",
       "85105                             I question sanitation.  \n",
       "70304                                           HORRIBLE  \n",
       "81660                              This stuff is amazing  \n",
       "49340                                           Terrible  \n",
       "71980                                      Good biscuits  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Text','Summary']]\n",
    "data.head()\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-marker",
   "metadata": {},
   "source": [
    "2개의 열이 남았네요. Text 열의 내용을 요약한 것이 Summary 열이다. \\\n",
    "여기서는 인공 신경망을 통해 Text 시퀀스를 입력받으면, Summary 시퀀스를 예측하도록 인공 신경망을 훈련시킬 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-physics",
   "metadata": {},
   "source": [
    "## 데이터 전처리하기 (1) - 데이터 정리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-department",
   "metadata": {},
   "source": [
    "### 중복 샘플과 NULL 값이 존재하는 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sunset-palace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-tablet",
   "metadata": {},
   "source": [
    "데이터프레임의 `drop_duplicates()`를 사용하면, 손쉽게 중복 샘플을 제거할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "signal-chair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-wrapping",
   "metadata": {},
   "source": [
    "데이터프레임에 Null 값이 있는지 확인하는 방법은 `.isnull().sum()`을 사용하면 알아볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "disabled-sector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-dynamics",
   "metadata": {},
   "source": [
    "Summary에 1개의 Null 값이 있으므로  `dropna()` 함수를 사용하여 제거하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "found-skating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-draft",
   "metadata": {},
   "source": [
    "### 텍스트 정규화와 불용어 제거\n",
    "\n",
    "예를 들어서 it'll은 it will과 같고, mustn't과 must not은 같은 표현이다. \\\n",
    "이런 경우 기계가 굳이 이들을 마치 다른 단어로 간주하게 해서 연산량을 늘리는 것보다는, \\\n",
    "기계 학습 전에 미리 같은 표현으로 통일시켜주는 것이 기계의 연산량을 줄일 수 있다. \\\n",
    "위와 같은 방법을 텍스트 처리에서는 텍스트 정규화(text normalization) 라고 한다.\n",
    "\n",
    "여기서는 텍스트 정규화를 위한 사전(dictionary)을 아래와 구성할 것이다.\n",
    "\n",
    "[정규화 사전 출처](https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "announced-demonstration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-scholar",
   "metadata": {},
   "source": [
    "`NLTK`에서 제공하는 불용어 리스트를 참조해, 샘플에서 불용어를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "oriental-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-alberta",
   "metadata": {},
   "source": [
    "NLTK에서 미리 정의하여 제공하고 있는 불용어는 총 179개 인 것을 확인할 수 있다. 이를 사용하여 불용어를 제거할 것이다.\\\n",
    "이 작업 외에도 \n",
    "- 모든 영어 문자는 소문자로 만들고, \n",
    "- 섞여있는 html 태그를 제거하고, \n",
    "- 정규 표현식을 통해 각종 특수문자를 제거해서 \n",
    "정말 필요한 내용만 잘 학습할 수 있도록 처리할 것이다.\n",
    "\n",
    "함수의 하단을 보면, `NLTK`를 이용해 불용어를 제거하는 파트가 있는데, \\\n",
    "이는 Text 전처리 시에서만 호출하고 이미 상대적으로 문장 길이가 짧은 Summary 전처리할 때는 호출하지 않을 예정이다.\\\n",
    "Abstractive한 문장 요약 결과문이 자연스러운 문장이 되려면 이 불용어들이 Summary에는 남아 있는 게 더 좋을 것 같다. \\\n",
    "이 처리를 위해서 함수의 인자로 `remove_stopwords`를 추가하고, if문을 추가했다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "backed-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-football",
   "metadata": {},
   "source": [
    "결과를 보면 기본적으로 \n",
    "- 모든 알파벳이 소문자로 변환되고, \n",
    "- `<br />`과 같은 html 태그가 제거 되었다. \n",
    "- 또한 (or finish)와 같은 괄호로 묶였던 단어 시퀀스가 제거된 것도 확인할 수 있다. \n",
    "- 특수문자가 제거되면서 영어만 남았다.\n",
    "\n",
    "이제 함수가 잘 작동하는 것을 확인했으니, 훈련 데이터 전체에 대해서 전처리를 수행해보자. \\\n",
    "이때, Text의 경우에는 불용어를 제거하고, Summary의 경우에는 불용어를 제거하지 않을 것이므로 따로 호출해서 진행해야 한다. \\\n",
    "먼저 Text를 전처리하고, 결과를 확인하기 위해서 상위 5개의 줄을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adjustable-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything bought great infact ordered twice third ordered wasfor mother father\n",
      "great way to start the day\n"
     ]
    }
   ],
   "source": [
    "## 위에서 만든 함수가 잘 작동하는지 예제 데이터를 사용해보자.\n",
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(preprocess_sentence(temp_text))\n",
    "print(preprocess_sentence(temp_summary, False))  # False : 불용어를 제거하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-ordinary",
   "metadata": {},
   "source": [
    "밑의 코드는 시간이 오래 걸린다... maybe 10분이상^_^!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(clean_text[:5])\n",
    "\n",
    "\n",
    "clean_summary = []\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(clean_summary[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-person",
   "metadata": {},
   "source": [
    "위에서 싱글프로세스로 사용하는 대신 밑에와 같이 멀티프로세싱을 사용하여 데이터 전처리의 소요 시간을 많이 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "civil-diagnosis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226.01202368736267  seconds\n",
      "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better'\n",
      " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo'\n",
      " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch'\n",
      " ...\n",
      " 'favorite brand korean ramen spicy used eating spicy food make sure use spice pack add egg soup makes great snack'\n",
      " 'like noodles although say spicy somewhat understatement one else family tolerates spicy well seeing looking forward extra little something palate disappointed completely honest usually drain liquid almost much'\n",
      " 'love noodle twice week amazing thing feel well cold hot bowl noodle cure upset stomach headache running nose may work definitely try']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.184816122055054  seconds\n",
      "['good quality dog food' 'not as advertised' 'delight says it all' ...\n",
      " 'great ramen' 'spicy'\n",
      " 'this spicy noodle cures my cold upset stomach and headache every time']\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp   # 멀티 프로세싱을 사용하기 위한 package\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial  # map을 할 때 함수에 여러 인자를 넣어줄 수 있도록 한다.\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# num_cores 만큼 쪼개진 데이터를 전처리하여 반환한다.\n",
    "def appendTexts(sentences, remove_stopwords):\n",
    "  texts = []\n",
    "  for s in sentences:\n",
    "    texts += preprocess_sentence(s, remove_stopwords),\n",
    "  return texts\n",
    "\n",
    "def preprocess_data(data, remove_stopwords=True):\n",
    "  start_time = time.time()\n",
    "  num_cores = mp.cpu_count()  # 컴퓨터의 코어 수\n",
    "\n",
    "  text_data_split = np.array_split(data, num_cores)  # 코어 수만큼 데이터를 배분하여 병렬적으로 처리한다\n",
    "  pool = Pool(num_cores)\n",
    "\n",
    "  processed_data = np.concatenate(pool.map(partial(appendTexts, remove_stopwords=remove_stopwords), text_data_split))  # 각자 작업한 데이터를 하나로 합쳐줍니다\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  print(time.time() - start_time, \" seconds\")\n",
    "  return processed_data\n",
    "\n",
    "clean_text = preprocess_data(data['Text'])  # text 전처리\n",
    "print(clean_text)\n",
    "\n",
    "clean_summary = preprocess_data(data['Summary'], remove_stopwords=False) # summary 전처리\n",
    "print(clean_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-tolerance",
   "metadata": {},
   "source": [
    "이제 Summary에 대해서 전처리 함수를 호출해 줄 때는, 불용어 제거를 수행하지 않는다는 의미에서 두 번째 인자로 False를 넣어준다.\n",
    "\n",
    "이렇게 텍스트 정제의 과정을 거친 후에는 다시 한번 빈(empty) 샘플이 생겼는지 확인해봐야 한다. \\\n",
    "정제 전에는 데이터가 존재했지만, 정제 과정에서 문장의 모든 단어가 사라지는 경우가 있다.\n",
    "\n",
    "보다 쉽게 확인하기 위해 데이터들을 데이터프레임에 재저장한다. \\\n",
    "빈(empty) 값을 가진 샘플들이 있다면, 모두 Null 값을 가진 샘플로 대체한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "desirable-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "moving-metallic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-faith",
   "metadata": {},
   "source": [
    "Summary 열에서 70개의 Null 값이 생겼다는 것은, \\\n",
    "원래는 단어가 있었는데, 정제 과정에서 모든 단어가 제거되어 빈 샘플이 70개 생겼다는 의미이다. \\\n",
    "이 샘플들은 모두 제거해주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "focused-lighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-conspiracy",
   "metadata": {},
   "source": [
    "## 데이터 전처리하기 (2) - 훈련데이터와 테스트데이터 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-entertainment",
   "metadata": {},
   "source": [
    "### 샘플의 최대 길이 정하기\n",
    "필요 없는 단어를 모두 솎아낸 데이터를 가지게 되었으니, 이제 훈련에 사용할 샘플의 최대 길이를 정해줄 차례이다.\n",
    "\n",
    "Text와 Summary의 최소, 최대, 평균 길이를 구하고 또한 길이 분포를 시각화해서 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "traditional-amsterdam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-royal",
   "metadata": {},
   "source": [
    "Summary의 경우 최소 길이가 1, 최대 길이가 28, 그리고 평균 길이가 4로 Text에 비해 상대적으로 길이가 짧다. \\\n",
    "그래프로 봤을 때에도 대체적으로 10이하의 길이를 가지고 있다.\n",
    "\n",
    "Text의 최대 길이와 Summary의 적절한 최대 길이를 임의로 정해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "finite-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 50\n",
    "summary_max_len = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-single",
   "metadata": {},
   "source": [
    "각각 50과 8로 정했는데 이 길이를 선택했을 때, \\\n",
    "얼마나 많은 샘플들을 자르지 않고 포함할 수 있는지 통계로 확인하는 편이 객관적으로 길이를 결정하는 데 도움이 된다. \\\n",
    "훈련 데이터와 샘플의 길이를 입력하면, 데이터의 몇 %가 해당하는지 계산하는 함수를 만들어서 좀 더 정확하게 판단해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "connected-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-fence",
   "metadata": {},
   "source": [
    "각각 50과 8로 패딩을 하게 되면 해당 길이보다 긴 샘플들은 내용이 잘리게 되는데, \\\n",
    "Text 열의 경우에는 약 23%의 샘플들이 내용이 망가지게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "characteristic-opportunity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-producer",
   "metadata": {},
   "source": [
    "우리는 정해진 길이에 맞춰 자르는 것이 아니라, 정해진 길이보다 길면 제외하는 방법으로 데이터를 정제해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "monetary-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 65818\n"
     ]
    }
   ],
   "source": [
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-federal",
   "metadata": {},
   "source": [
    "### 시작 토큰과 종료 토큰 추가하기\n",
    "디코더는 시작 토큰을 입력받아 문장을 생성하기 시작하고, 종료 토큰을 예측한 순간에 문장 생성을 멈춘다.\n",
    "\n",
    "seq2seq 훈련을 위해서는 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가할 필요가 있다. \\\n",
    "이번 실습에서는 시작 토큰은 `sostoken`, 종료 토큰은 `eostoken`이라 임의로 명명하고 앞, 뒤로 추가할 것이다. \\\n",
    "디코더의 입력에 해당하면서 시작 토큰이 맨 앞에 있는 문장의 이름을 `decoder_input`, \\\n",
    "디코더의 출력 또는 레이블에 해당되면서 종료 토큰이 맨 뒤에 붙는 문장의 이름을 `decoder_target`이라고 정해준다. \\\n",
    "두 개의 문장 모두 Summary 열로부터 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "asian-array",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "preceding-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-proportion",
   "metadata": {},
   "source": [
    "이제 훈련 데이터와 테스트 데이터를 분리할 것이다.\n",
    "\n",
    "훈련 데이터와 테스트 데이터를 분리하는 방법은 분리 패키지를 사용하는 방법, \\\n",
    "또는 직접 코딩을 통해서 분리하는 방법 등 여러 가지 방법이 있을 텐데 여기서는 직접 해보자. \\\n",
    "우선, `encoder_input`과 크기와 형태가 같은 순서가 섞인 정수 시퀀스를 만들어 줄것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fleet-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43253 38706 61071 ... 34062 17347 44718]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sharp-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-relief",
   "metadata": {},
   "source": [
    "이제 섞인 데이터를 8:2의 비율로 훈련 데이터와 테스트 데이터로 분리해주자. \\\n",
    "전체 데이터의 크기에서 0.2를 곱해서 테스트 데이터의 크기를 정의해 주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "executive-editing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13163\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "charged-legislation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52655\n",
      "훈련 레이블의 개수 : 52655\n",
      "테스트 데이터의 개수 : 13163\n",
      "테스트 레이블의 개수 : 13163\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-design",
   "metadata": {},
   "source": [
    "## 데이터 전처리하기 (3) - 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-basket",
   "metadata": {},
   "source": [
    "### 단어 집합(vocabulary) 만들기 및 정수 인코딩\n",
    "이제 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 모두 정수로 바꿔야한다. \\\n",
    "이를 위해서는 각 단어에 고유한 정수를 맵핑하는 작업이 필요하다. 이 과정을 **단어 집합(vocabulary)** 을 만든다고 표현한다. \\\n",
    "훈련 데이터에 대해서 단어 집합을 만들어보자. \n",
    "\n",
    "우선, 원문에 해당되는 `encoder_input_train`에 대해서 단어 집합을 만들자. \\\n",
    "Keras의 토크나이저를 사용하면, 입력된 훈련 데이터로부터 단어 집합을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accessible-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-nicholas",
   "metadata": {},
   "source": [
    "이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었다. \\\n",
    "현재 생성된 단어 집합은 `src_tokenizer.word_index`에 저장되어 있다. \\\n",
    "그런데 우리는 이렇게 만든 단어 집합에 있는 모든 단어를 사용하는 것이 아니라, 빈도수가 낮은 단어들은 훈련 데이터에서 제외하고 진행하려고 한다.\n",
    "\n",
    "등장 빈도수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해보자.\n",
    "\n",
    "`src_tokenizer.word_counts.items()`에는 단어와 각 단어의 등장 빈도수가 저장돼 있는데, 이를 통해서 통계적인 정보를 확인해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "attended-terrorism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 32089\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 23852\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8237\n",
      "단어 집합에서 희귀 단어의 비율: 74.33076755274392\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.3953436455020385\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-prediction",
   "metadata": {},
   "source": [
    "`encoder_input_train`에는 3만여 개의 단어가 있다. \n",
    "\n",
    "- 등장 빈도가 threshold 값인 7회 미만, 즉, 6회 이하인 단어들은 단어 집합에서 70% 이상\n",
    "- 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 3.39%\n",
    "\n",
    "그래서 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거해주자. \\\n",
    "위에서 이를 제외한 단어 집합의 크기를 8천여 개로 계산했는데, 이와 비슷한 값으로 어림잡아 단어 집합의 크기를 8,000으로 제한해보자. \\\n",
    "토크나이저를 정의할 때 num_words의 값을 정해주면, 단어 집합의 크기를 제한할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hollywood-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-weekly",
   "metadata": {},
   "source": [
    "`texts_to_sequences()`는 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩을 수행한다. \\\n",
    "현재 단어 집합의 크기를 8,000으로 제한했으니까 이제 8,000이 넘는 숫자들은 정수 인코딩 후에는 데이터에 존재하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "incorporated-logging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35, 685, 1, 9, 8, 29, 36, 8, 9, 24, 72, 196, 508, 2139, 8, 388, 7, 2, 43, 114, 820, 668, 239, 976, 794, 10, 42, 43, 465, 8, 738], [45, 5, 107, 822, 186, 675, 1542, 166, 1107, 95, 5, 189, 85, 1552, 176, 3893, 109, 23, 5, 14, 107, 16], [1, 770, 772, 515, 640, 21, 1338, 62, 10, 25, 96, 209, 155, 304, 699, 67, 515, 640, 723, 27, 20, 1499, 268, 610, 81, 530, 1406, 57, 2782, 10, 17, 561, 515, 1345, 2, 159, 679, 62, 200, 10, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-restaurant",
   "metadata": {},
   "source": [
    "케라스의 토크나이저를 사용하여 decoder_input_train을 입력으로 전체 단어 집합과 각 단어에 대한 빈도수를 계산해요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "vanilla-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-title",
   "metadata": {},
   "source": [
    "이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되어있다. 이는 `tar_tokenizer.word_index`에 저장되어 있다. \n",
    "\n",
    "`tar_tokenizer.word_counts.items()`에는 단어와 각 단어의 등장 빈도수가 저장돼 있는데, \\\n",
    "이를 통해서 통계적인 정보를 얻어서, 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "instructional-jones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10485\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8128\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2357\n",
      "단어 집합에서 희귀 단어의 비율: 77.52026704816404\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.955084749414964\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-colonial",
   "metadata": {},
   "source": [
    "등장 빈도가 5회 이하인 단어들은 단어 집합에서 약 77% 정도가 있다. \\\n",
    "하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 수치인 5.89%밖에 되지 않으므로, \\\n",
    "아까 했던 것과 동일하게 이 단어들은 모두 제거해주자. \\\n",
    "어림잡아 2,000을 단어 집합의 크기로 제한한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rocky-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 1109, 1940, 13, 103], [1, 1941, 1675, 699, 668], [1, 50, 214, 5, 261, 1942], [1, 82], [1, 15, 38, 63, 111]]\n",
      "target\n",
      "decoder  [[1109, 1940, 13, 103, 2], [1941, 1675, 699, 668, 2], [50, 214, 5, 261, 1942, 2], [82, 2], [15, 38, 63, 111, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-fairy",
   "metadata": {},
   "source": [
    "요약문에서 길이가 0이 된 샘플들의 인덱스를 받아올 것인데, 여기서 주의할 점은 요약문인 `decoder_input`에는 `sostoken` 또는 `decoder_target`에는 `eostoken`이 추가된 상태이고, 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플 수와 동일하게 매우 높으므로 단어 집합 제한에도 삭제되지 않는다. \\\n",
    "그래서 이제 길이가 0이 된 요약문의 실제 길이는 1로 나올 것이다.\n",
    "\n",
    "훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 1인 경우의 인덱스를 각각 drop_train과 drop_test에 라는 변수에 저장한 후, 이 샘플들은 모두 삭제할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "instructional-belarus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1266\n",
      "삭제할 테스트 데이터의 개수 : 337\n",
      "훈련 데이터의 개수 : 51389\n",
      "훈련 레이블의 개수 : 51389\n",
      "테스트 데이터의 개수 : 12826\n",
      "테스트 레이블의 개수 : 12826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-population",
   "metadata": {},
   "source": [
    "### 패딩하기\n",
    "텍스트 시퀀스를 정수 시퀀스로 변환했다면, 이제 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업을 해줘야 한다. \\\n",
    "아까 정해두었던 최대 길이로 패딩 해줄 것이며, 최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 최대 길이로 길이를 맞춰주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "incident-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-current",
   "metadata": {},
   "source": [
    "## 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "thermal-allergy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-sandwich",
   "metadata": {},
   "source": [
    "임베딩 벡터의 차원은 128로 정의하고, hidden state의 크기를 256으로 정의했어요. hidden state는 LSTM에서 얼만큼의 수용력(capacity)를 가질지를 정하는 파라미터에요. 이 파라미터는 LSTM의 용량의 크기나, LSTM에서의 뉴런의 개수라고 이해하면 돼요. 다른 신경망과 마찬가지로, 무조건 용량을 많이 준다고 해서 성능이 반드시 올라가는 것은 아니에요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "automated-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-married",
   "metadata": {},
   "source": [
    "디코더의 임베딩 층과 LSTM을 설계하는 것은 인코더와 거의 동일하다. \\\n",
    "하지만 LSTM의 입력을 정의할 때, `initial_state`의 인자값으로 인코더의 `hidden state`와 `cell state`의 값을 넣어줘야 한다.\n",
    "\n",
    "디코더의 출력층을 설계해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "roman-trustee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-portland",
   "metadata": {},
   "source": [
    "### 어텐션 메커니즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-fiber",
   "metadata": {},
   "source": [
    "깃허브에 공개돼 있는 어텐션 함수를 다운로드한다. \\\n",
    "이제 경로에 `attention.py` 파일이 생겼으니, 어텐션 메커니즘을 불러와서 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "thousand-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "centered-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,276,432\n",
      "Trainable params: 4,276,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-confusion",
   "metadata": {},
   "source": [
    "## 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "entitled-christianity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "201/201 [==============================] - 146s 673ms/step - loss: 3.1388 - val_loss: 2.4257\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 133s 662ms/step - loss: 2.4166 - val_loss: 2.3081\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 132s 655ms/step - loss: 2.2772 - val_loss: 2.1658\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 132s 655ms/step - loss: 2.1365 - val_loss: 2.0782\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 130s 649ms/step - loss: 2.0378 - val_loss: 2.0267\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 129s 641ms/step - loss: 1.9747 - val_loss: 1.9798\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 128s 638ms/step - loss: 1.9126 - val_loss: 1.9535\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 128s 638ms/step - loss: 1.8509 - val_loss: 1.9238\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 1.8226 - val_loss: 1.9071\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 129s 641ms/step - loss: 1.7791 - val_loss: 1.8952\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 128s 639ms/step - loss: 1.7552 - val_loss: 1.8855\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 129s 639ms/step - loss: 1.7187 - val_loss: 1.8762\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 1.6882 - val_loss: 1.8667\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 1.6545 - val_loss: 1.8652\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 128s 638ms/step - loss: 1.6322 - val_loss: 1.8584\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 128s 639ms/step - loss: 1.6068 - val_loss: 1.8524\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 129s 639ms/step - loss: 1.5864 - val_loss: 1.8534\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 129s 640ms/step - loss: 1.5570 - val_loss: 1.8553\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-lying",
   "metadata": {},
   "source": [
    "'조기 종료'를 뜻하는 EarlyStopping은 특정 조건이 충족되면 훈련을 멈추는 역할을 한다.\n",
    "```\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "```\n",
    "\n",
    "위 코드에서는 val_loss(검증 데이터의 손실)을 관찰하다가, 검증 데이터의 손실이 줄어들지 않고 증가하는 현상이 2회(patience=2) 관측되면 학습을 멈추도록 설정했다. EarlyStopping이 작동한다면 epochs가 아무리 크게 설정되어 있어도 모델 훈련을 최적점에서 멈출 수 있게 된다.\n",
    "\n",
    "[EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "gross-majority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-vertical",
   "metadata": {},
   "source": [
    "## 인퍼런스 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "historic-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-harvest",
   "metadata": {},
   "source": [
    "seq2seq는 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행해야 한다.\n",
    "\n",
    "훈련 단계에서는 디코더의 입력부에 정답이 되는 문장 전체를 한꺼번에 넣고 디코더의 출력과 한 번에 비교할 수 있으므로, \n",
    "인코더와 디코더를 엮은 통짜 모델 하나만 준비했다.\n",
    "\n",
    "그러나 정답 문장이 없는 인퍼런스 단계에서는 만들어야 할 문장의 길이만큼 디코더가 반복 구조로 동작해야 하기 때문에 부득이하게 인퍼런스를 위한 모델 설계를 별도로 해주어야 하며, 이때는 인코더 모델과 디코더 모델을 분리해서 설계한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "involved-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-integer",
   "metadata": {},
   "source": [
    "어텐션 메커니즘을 사용하는 출력층을 설계해주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "seasonal-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-lawrence",
   "metadata": {},
   "source": [
    "인퍼런스 단계에서 단어 시퀀스를 완성하는 함수를 만들어주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "premium-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-heavy",
   "metadata": {},
   "source": [
    "## 모델 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-editing",
   "metadata": {},
   "source": [
    "주어진 정수 시퀀스를 텍스트 시퀀스로 변환하는 함수를 만들어보자. \n",
    "함수를 만들 때, Text의 정수 시퀀스에서는 패딩을 위해 사용되는 숫자 0을 제외하고 Summary의 정수 시퀀스에서는 숫자 0, 시작 토큰의 인덱스, 종료 토큰의 인덱스를 출력에서 제외하도록 만들 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bearing-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "historic-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : pretty gift basket contains samples morsels great go along cup coffee pretty basket gold ribbon contains following kettle fresh chocolate fudge truffle cookies belgian cookies chocolate biscotti cashew cookies sweet butter cookies chocolate chip cookies would make great gift friend \n",
      "실제 요약 : nice gift \n",
      "예측 요약 :  great for the price\n",
      "\n",
      "\n",
      "원문 : best chocolates ever absolutely incredible chocolate good rare us also make excellent gift wish people would give leonidas \n",
      "실제 요약 : incredible \n",
      "예측 요약 :  best popcorn ever\n",
      "\n",
      "\n",
      "원문 : recently professional facial interested finding mask use home expensive jar product facial selling scent product quite lovely uses feel slight difference skin reason give full stars wish stronger \n",
      "실제 요약 : skin \n",
      "예측 요약 :  good product\n",
      "\n",
      "\n",
      "원문 : like really favorite flavors coffee may package looking ones like best like greatest hits package filler \n",
      "실제 요약 : favorite flavors \n",
      "예측 요약 :  best coffee ever\n",
      "\n",
      "\n",
      "원문 : good value money top line stash tea \n",
      "실제 요약 : double tea \n",
      "예측 요약 :  good tea\n",
      "\n",
      "\n",
      "원문 : bought brand hot chocolate cups cause best review amazon time kids love especially like peppermint flavor cannot see way get except mixed box get milk chocolate dark chocolate peppermint flavors tried times weak taste make lowest setting keurig think around ounces still weak perfect like strong chocolate flavor \n",
      "실제 요약 : kids love it me not so much \n",
      "예측 요약 :  nice\n",
      "\n",
      "\n",
      "원문 : exactly looking bacon england sooo much better usa shipped quickly fresh \n",
      "실제 요약 : first rate \n",
      "예측 요약 :  great\n",
      "\n",
      "\n",
      "원문 : begun trouble finding crystal light peach tea grocery stores tried amazon bit surprised find family loves peach tea batch runs buy \n",
      "실제 요약 : peach tea \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : kitty little messy needed one local pet supply stores carry happy found seller amazon com continue saved product durable even wash washing machine cold water \n",
      "실제 요약 : great product \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : product pretty delicious love using stevia instead fake sugars natural product seems less likely cause cancer wish things would use instead \n",
      "실제 요약 : tasty \n",
      "예측 요약 :  not as good as the original\n",
      "\n",
      "\n",
      "원문 : month old lab months loves want feed chicken products opted lamb coat shiny lots energy looking alternative chicken based puppy food highly recommend one \n",
      "실제 요약 : lab puppy loves it \n",
      "예측 요약 :  my dog loves it\n",
      "\n",
      "\n",
      "원문 : good right green food item three year old mouth however bit salty buy way cheaper asian food grocery \n",
      "실제 요약 : good but \n",
      "예측 요약 :  good but not great\n",
      "\n",
      "\n",
      "원문 : impressed taste first thing almond cashew bar reminded pecan pie yet even pecans pros taste good expensive larger bar says made australia \n",
      "실제 요약 : impressed but expensive \n",
      "예측 요약 :  not too sweet\n",
      "\n",
      "\n",
      "원문 : become fitness queen working five six days week trainer told workout need protein trying various protein shakes health club vitamin world finally settled shakes special shakes enjoyable meet purpose working order bulk easier \n",
      "실제 요약 : need my protein \n",
      "예측 요약 :  best oatmeal ever\n",
      "\n",
      "\n",
      "원문 : bought reading reviews making pork wow tastes good impress amount flavor salt pork yummy got try slow cooking method liquid smoke \n",
      "실제 요약 : perfect pork \n",
      "예측 요약 :  great salt\n",
      "\n",
      "\n",
      "원문 : best flavor box salt pepper bbq flavor flavor \n",
      "실제 요약 : get the salt and pepper box instead \n",
      "예측 요약 :  good salt\n",
      "\n",
      "\n",
      "원문 : product super dog tends wolf food wonder went putting cup food thing dinner time makes work tough dog hard mine last broken part means indestructible trying train bring say bottle see \n",
      "실제 요약 : excellent product \n",
      "예측 요약 :  dog food\n",
      "\n",
      "\n",
      "원문 : ordered enjoying blend seemingly forever order decent cup use times amount coffee need using blend \n",
      "실제 요약 : good coffee just not the best \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : bought cats decided cut way back amount corn products eat three liked ordering price reasonable \n",
      "실제 요약 : fine cat food \n",
      "예측 요약 :  cats love it\n",
      "\n",
      "\n",
      "원문 : crackers absolutely delicious snack ever addictive unfortunately come think order \n",
      "실제 요약 : the best \n",
      "예측 요약 :  great chips\n",
      "\n",
      "\n",
      "원문 : many years drinking twinings irish breakfast tea like tea much better add sugar creamer changes taste bit like tea way give try \n",
      "실제 요약 : better than twinings \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : absolutely wonderful one pie pan fills ready cook pricey worth cost \n",
      "실제 요약 : mm \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : bars fantastic lots protein low sugar taste like cardboard big fan going get shipped every save \n",
      "실제 요약 : wow great bars \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : bought pack amazon special normally drink medium blends stay away flavored wife likes flavored coffees favorite pumpkin spice green mountain found flavor artificial almost think tastes bit like maple syrup sweet would better less flavor tried making largest cup setting waters coffee like flavored liquid coffee creamers might something like \n",
      "실제 요약 : not my favorite syrup artificial flavor \n",
      "예측 요약 :  nice flavor\n",
      "\n",
      "\n",
      "원문 : say blew mouth away good old made snack time best time \n",
      "실제 요약 : more than yummy \n",
      "예측 요약 :  yummy\n",
      "\n",
      "\n",
      "원문 : hot sauce delicious put chinese food mexican food macaroni cheese everything used dipping sauce pork chops chicken fingers corn dogs looking something really tasty kick try stuff \n",
      "실제 요약 : put this on all your food \n",
      "예측 요약 :  great sauce\n",
      "\n",
      "\n",
      "원문 : sitting watching enjoying latte national spirit giving praise espresso ground great moved california dissapointed affordable coffee nyc missed peet tops flavor body godsend arrive fresh ready go thanks \n",
      "실제 요약 : great taste of home \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : great dunkin donuts ground coffee prefer plain cream easy storage hassle compared powdered creamers make mess \n",
      "실제 요약 : cream for coffee lovers \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : great much hurry cook breakfast make lunch flavor rich chocolatey add cup milk blend good go \n",
      "실제 요약 : great product for busy mom \n",
      "예측 요약 :  great for\n",
      "\n",
      "\n",
      "원문 : use maple syrup almost sweetening baking granola whatever grade organic syrup wonderful also received within days ordering \n",
      "실제 요약 : delicious and timely \n",
      "예측 요약 :  maple syrup\n",
      "\n",
      "\n",
      "원문 : mother boys preschool teacher training th half marathon safe say need energy help get sport shot first natural energy shot found gives right amount energy boost without jitters love made coconut water fantastic way replenish potassium electrolytes miles longer run \n",
      "실제 요약 : an all natural alternative with coconut water \n",
      "예측 요약 :  great for training\n",
      "\n",
      "\n",
      "원문 : try mi never go back first introduced australia would eat almost everyday give two thumbs five stars \n",
      "실제 요약 : best damn noodles ever \n",
      "예측 요약 :  yummy\n",
      "\n",
      "\n",
      "원문 : like like pick walgreens little four dollars well \n",
      "실제 요약 : cute \n",
      "예측 요약 :  not as good as the\n",
      "\n",
      "\n",
      "원문 : love cincinnati chili purchased seasoning mix really like directions easy since cannot get famous dish california next best thing visiting one cincinnati chili like spaghetti onions cheddar cheese lots cheddar cheese yum \n",
      "실제 요약 : chili \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : taste surprisingly good would compare cross sunflower seeds cashews eat plain morning found blend oatmeal well \n",
      "실제 요약 : good stuff \n",
      "예측 요약 :  great flavor\n",
      "\n",
      "\n",
      "원문 : tried brand green tea drinking celestial seasonings twinings green teas one strong strange flavor almost tasted like meat broth rather tea think another reviewer described burnt flavor may gotten right tried immediately switched back twinings recently tried second bag thinking maybe first time pour away \n",
      "실제 요약 : very strange flavor \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : mrs may pumpkin crunch best bunch like pumpkin seeds slightly sweet cubes make think eating cake great snacking even dessert truly wonderful product gluten free bargain \n",
      "실제 요약 : mrs may pumpkin crunch is terrific \n",
      "예측 요약 :  great for gluten free\n",
      "\n",
      "\n",
      "원문 : worked since first day started use getting ml everytime fenugreek increased ml wasnt lot milk say worked added tea milk supply even bigger guess pills plus tea works even better fenugreek pills good luck \n",
      "실제 요약 : it works \n",
      "예측 요약 :  works for me\n",
      "\n",
      "\n",
      "원문 : good tasting honey good price honey use baking adding hot tea prefer use raw honey anything going cooked heated high temp \n",
      "실제 요약 : excellent tast excellent price \n",
      "예측 요약 :  good stuff\n",
      "\n",
      "\n",
      "원문 : great gf kid whole family hooked chocolate chip like oatmeal cinnamon lot great come individual packs full sized cookies pretty healthy natural ingredients could wrong great recommend \n",
      "실제 요약 : thank you dr lucy \n",
      "예측 요약 :  great gluten free snack\n",
      "\n",
      "\n",
      "원문 : great variety pack skeptical blueberry fantastic made whole house smell like blueberry pie \n",
      "실제 요약 : cup variety \n",
      "예측 요약 :  yummy\n",
      "\n",
      "\n",
      "원문 : bought sister christmas loved afraid would eat took box work within hour finished \n",
      "실제 요약 : yummy \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : say true version gummy bears living germany early \n",
      "실제 요약 : yummy gummy \n",
      "예측 요약 :  good stuff\n",
      "\n",
      "\n",
      "원문 : words come mind amazing wonderful delicious fantastic delightful thank snyder pieces bravo next best thing actual buffalo chicken wing \n",
      "실제 요약 : get some \n",
      "예측 요약 :  best ever\n",
      "\n",
      "\n",
      "원문 : nice healthy sweet snack quick breakfast taste quite good always keep hand \n",
      "실제 요약 : love them \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : tea decaf tastes wonderful real smooth taste tummy upset helps love tea \n",
      "실제 요약 : wonderful \n",
      "예측 요약 :  my favorite tea\n",
      "\n",
      "\n",
      "원문 : bought help fall asleep works although tastes smell really bad make sure put sort glass jar prevent kitchen found added cream tea would help taste still pretty bad \n",
      "실제 요약 : bad taste but helps you fall \n",
      "예측 요약 :  works well\n",
      "\n",
      "\n",
      "원문 : better could ever hoped far weight watcher plus points goes lot package worth calories points \n",
      "실제 요약 : super yum \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : love mint eating since kid wasa favorite movie theater item problem awhile supplier sends stale hard place list companies longer order anything time amazon direct supplier anything always good results \n",
      "실제 요약 : first if fresh \n",
      "예측 요약 :  great product but\n",
      "\n",
      "\n",
      "원문 : really quite good used cup leaves coffee maker full pot water add almost cup sugar throw fridge top half half got exactly serving thai restaurant order product \n",
      "실제 요약 : works for me \n",
      "예측 요약 :  good but not great\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-valentine",
   "metadata": {},
   "source": [
    "## 추출적 요약 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-salem",
   "metadata": {},
   "source": [
    "앞서 seq2seq를 통해서 추상적 요약을 진행해봤으며, \\\n",
    "텍스트 요약에는 __추상적 요약__ 외에도 이미 본문에 존재하는 단어구, 문장을 뽑아서 요약으로 삼는 __추출적 요약__ 방법도 있다.\n",
    "\n",
    "패키지 `Summa`에서는 추출적 요약을 위한 모듈인 `summarize`를 제공하고 있어 아주 간단하게 실습이 가능하다. \\\n",
    "영화 매트릭스 시놉시스를 요약해보면서 `summarize` 사용법을 간단하게 익혀보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-myanmar",
   "metadata": {},
   "source": [
    "### 패키지 설치\n",
    "```\n",
    "$ pip install summa\n",
    "$ pip list | grep summa\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "mysterious-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The screen is filled with green, cascading code which gives way to the title, The Matrix.\r\n",
      "\r\n",
      "A phone rings and text appears on the screen: \"Call trans opt: received. 2-19-98 13:24:18 REC: Log>\" As a conversation takes place between Trinity (Carrie-Anne Moss) and Cypher (Joe Pantoliano), two free humans, a table of random green numbers are being scanned and individual numbers selected, creating a series of digits not unlike an ordinary phone number, as if a code is being deciphered or a call is being traced.\r\n",
      "\r\n",
      "Trinity discusses some unknown person. Cypher taunts Trinity, suggesting she enjoys watching him. Trinity counters that \"Morpheus (Laurence Fishburne) says he may be 'the One',\" just as the sound of a number being selected alerts Trinity that someone may be tracing their call. She ends the call.\r\n",
      "\r\n",
      "Armed policemen move down a darkened, decrepit hallway in the Heart O' the City Hotel, their flashlight beam bouncing just ahead of them. They come to room 303, kick down the door and find a woman dressed in black, facing away from them. It's Trinity. She brings her hands up from the laptop she's working on at their command.\r\n",
      "\r\n",
      "Outside the hotel a car drives up and three agents appear in neatly pressed black suits. They are Agent Smith (Hugo Weaving), Agent Brown (Paul Goddard), and Agent Jones (Robert Taylor). Agent Smith and the presiding police lieutenant argue. Agent Smith admonishes the policeman that they were given specific orders to contact the agents first, for their\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "## 매트릭스 시놉시스를 가져온다.\n",
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text\n",
    "\n",
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-gamma",
   "metadata": {},
   "source": [
    "### summarize 사용하기\n",
    "Summa의 summarize()의 인자들에 대해 알아보자.\n",
    "\n",
    "- text (str) : 요약할 테스트.\n",
    "- ratio (float, optional) – 요약문에서 원본에서 선택되는 문장 비율. 0~1 사이값\n",
    "- words (int or None, optional) – 출력에 포함할 단어 수.\n",
    "- 만약, ratio와 함께 두 파라미터가 모두 제공되는 경우 ratio는 무시한다.\n",
    "- split (bool, optional) – True면 문장 list / False는 조인(join)된 문자열을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-statistics",
   "metadata": {},
   "source": [
    "`Summa`의 `summarize`는 문장 토큰화를 별도로 하지 않더라도 내부적으로 문장 토큰화를 수행한다. \\\n",
    "때문에 문장 구분이 되어있지 않은 원문을 바로 입력으로 넣을 수 있다. \\\n",
    "비율을 적게 주어서 요약문으로 선택되는 문장의 개수를 줄여보겠다. \\\n",
    "원문의 0.005%만을 출력하도록 설정해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "under-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n",
      "Summary:\n",
      "['Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.', 'Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.']\n",
      "Summary:\n",
      "Trinity takes Neo to Morpheus.\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005))\n",
    "\n",
    "## 만약 리스트로 출력 결과를 받고 싶다면 split 인자의 값을 True로 하면 된다.\n",
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True))\n",
    "\n",
    "## 단어의 수로 요약문의 크기를 조절할 수도 있다. 단어를 50개만 선택하도록 해보자.\n",
    "print('Summary:')\n",
    "print(summarize(text, words=50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
